{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kf_QmyI_rMBg",
   "metadata": {
    "id": "kf_QmyI_rMBg"
   },
   "source": [
    "# KRX-Bench 합성 데이터셋 생성 with raw text 튜토리얼\n",
    "\n",
    "- **litellm**은 다양한 LLM API를 OpenAI API로 통합하여 사용할 수 있는 라이브러리입니다.\n",
    "- 본 튜토리얼에서는 OpenAI의 `gpt-4o-mini-2024-07-18` 모델을 활용하여 금융 관련 고품질 raw text 데이터셋 `alvanlii/finance-textbooks`을 QA Instruction 데이터셋으로 변환하는 방법에 대해 다룹니다.\n",
    "- 데이터 생성 파이프라인:\n",
    " 1. LLM에게 샘플링된 raw text를 입력으로 해서 질문 세트 생성\n",
    " 2. 생성된 질문 세트에 대한 답변 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uBGB4FQtvOfT",
   "metadata": {
    "id": "uBGB4FQtvOfT"
   },
   "source": [
    "## 1. litellm 설치 및 환경 설정\n",
    "- 필요 라이브러리: pandas, datasets, random, litellm, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "R_Rt-W7rbDN_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_Rt-W7rbDN_",
    "outputId": "5445ce2c-23fd-4b4a-ed70-ceaba4f7e59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.54.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.7.0-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jake\\onedrive - 제주대학교\\바탕 화면\\project\\krx\\krx-llm\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jake\\onedrive - 제주대학교\\바탕 화면\\project\\krx\\krx-llm\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jake\\onedrive - 제주대학교\\바탕 화면\\project\\krx\\krx-llm\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached openai-1.54.0-py3-none-any.whl (389 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.7.0-cp312-none-win_amd64.whl (199 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: pytz, tzdata, typing-extensions, tqdm, sniffio, numpy, jiter, idna, h11, distro, certifi, annotated-types, pydantic-core, pandas, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 certifi-2024.8.30 distro-1.9.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 idna-3.10 jiter-0.7.0 numpy-2.1.3 openai-1.54.0 pandas-2.2.3 pydantic-2.9.2 pydantic-core-2.23.4 pytz-2024.2 sniffio-1.3.1 tqdm-4.66.6 typing-extensions-4.12.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "V5uLqJoZbLxf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5uLqJoZbLxf",
    "outputId": "7709b4db-d271-4a7c-99a8-30b024e31e73"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# API 키 입력 (실행 시 입력하도록 설정)\n",
    "\n",
    "api_key = input(\"API 키를 입력하세요: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eX5d3BnsbS-1",
   "metadata": {
    "id": "eX5d3BnsbS-1"
   },
   "outputs": [],
   "source": [
    "# OpenAI 클라이언트 설정\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 배치 크기 설정\n",
    "BATCH_SIZE = 30  # 한 번에 처리할 요청 수\n",
    "DELAY = 3  # 배치 간 대기 시간(초)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70fde1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('final.json', 'r', encoding='utf-8'))\n",
    "ap = data.values()\n",
    "data = pd.DataFrame(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad8983f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table>\\n<colgroup>\\n<col>\\n<col>\\n<col>\\n<col>\\n</colgroup>\\n<thead>\\n<tr>\\n<th>매출유형</th>\\n<th>품 목</th>\\n<th>매출액</th>\\n<th>비 중(%)</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>방송공연</td>\\n<td>방송출연, 콘서트, 각종행사등</td>\\n<td>5,375</td>\\n<td>46.42</td>\\n</tr>\\n<tr>\\n<td>공연,기획</td>\\n<td>당사, 타사의 콘서트의 기획,진행등</td>\\n<td>6,189</td>\\n<td>53.45</td>\\n</tr>\\n<tr>\\n<td>기 타</td>\\n<td>영상제작등</td>\\n<td>15</td>\\n<td>0.13</td>\\n</tr>\\n<tr>\\n<td>합  계</td>\\n<td>11,579</td>\\n<td>100</td>\\n</tr>\\n</tbody>\\n</table>\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['table'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a69e0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.replace('\\xa0', ''))\n",
    "for table in data['table']:\n",
    "    for i in range(len(table)):\n",
    "        table[i] = table[i].replace('\\xa0', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6199b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data.to_dict(), f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batch\n",
    "\n",
    "def make_batch(data, batch_size):\n",
    "    batch = []\n",
    "    for row in data.itertuples():\n",
    "        if row.table!= []:\n",
    "            for table in row.table:\n",
    "                if len(table) < 2048:\n",
    "                    \n",
    "        batch.append(row.text)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OH6wBYKKbie7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH6wBYKKbie7",
    "outputId": "3dcd9c0e-0da2-4a0b-b7cc-dcfca0deefe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 파일 로딩 중...\n",
      "처리 시작...\n",
      "텍스트 샘플링 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 125486.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 처리 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 1/167: 질문 생성 중...\n",
      "배치 1/167: 답변 생성 중...\n",
      "배치 1 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  1%|          | 1/167 [02:45<7:38:41, 165.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 2/167: 질문 생성 중...\n",
      "배치 2/167: 답변 생성 중...\n",
      "배치 2 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  1%|          | 2/167 [05:44<7:57:20, 173.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 3/167: 질문 생성 중...\n",
      "배치 3/167: 답변 생성 중...\n",
      "배치 3 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  2%|▏         | 3/167 [08:44<8:01:56, 176.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 4/167: 질문 생성 중...\n",
      "배치 4/167: 답변 생성 중...\n",
      "배치 4 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  2%|▏         | 4/167 [11:51<8:10:27, 180.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 5/167: 질문 생성 중...\n",
      "배치 5/167: 답변 생성 중...\n",
      "배치 5 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  3%|▎         | 5/167 [15:03<8:18:21, 184.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 6/167: 질문 생성 중...\n",
      "배치 6/167: 답변 생성 중...\n",
      "배치 6 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  4%|▎         | 6/167 [17:57<8:05:48, 181.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 7/167: 질문 생성 중...\n",
      "배치 7/167: 답변 생성 중...\n",
      "배치 7 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  4%|▍         | 7/167 [20:45<7:51:29, 176.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 8/167: 질문 생성 중...\n",
      "배치 8/167: 답변 생성 중...\n",
      "배치 8 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  5%|▍         | 8/167 [23:50<7:55:34, 179.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 9/167: 질문 생성 중...\n",
      "배치 9/167: 답변 생성 중...\n",
      "배치 9 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  5%|▌         | 9/167 [26:15<7:24:00, 168.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 10/167: 질문 생성 중...\n",
      "배치 10/167: 답변 생성 중...\n",
      "배치 10 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  6%|▌         | 10/167 [28:42<7:03:59, 162.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 11/167: 질문 생성 중...\n",
      "배치 11/167: 답변 생성 중...\n",
      "배치 11 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  7%|▋         | 11/167 [31:22<6:59:44, 161.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 12/167: 질문 생성 중...\n",
      "배치 12/167: 답변 생성 중...\n",
      "배치 12 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  7%|▋         | 12/167 [34:17<7:07:42, 165.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 13/167: 질문 생성 중...\n",
      "배치 13/167: 답변 생성 중...\n",
      "배치 13 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  8%|▊         | 13/167 [36:56<7:00:01, 163.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 14/167: 질문 생성 중...\n",
      "배치 14/167: 답변 생성 중...\n",
      "배치 14 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  8%|▊         | 14/167 [40:03<7:14:48, 170.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 15/167: 질문 생성 중...\n",
      "배치 15/167: 답변 생성 중...\n",
      "배치 15 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  9%|▉         | 15/167 [43:20<7:32:29, 178.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 16/167: 질문 생성 중...\n",
      "배치 16/167: 답변 생성 중...\n",
      "배치 16 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 10%|▉         | 16/167 [46:09<7:21:51, 175.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 17/167: 질문 생성 중...\n",
      "배치 17/167: 답변 생성 중...\n",
      "배치 17 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 17/167 [49:13<7:25:26, 178.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 18/167: 질문 생성 중...\n",
      "배치 18/167: 답변 생성 중...\n",
      "배치 18 완료. 3초 대기 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 11%|█         | 18/167 [51:48<7:05:17, 171.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "배치 19/167: 질문 생성 중...\n",
      "배치 19/167: 답변 생성 중...\n"
     ]
    }
   ],
   "source": [
    "def batch_completion(messages_list, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"\n",
    "    배치로 OpenAI API 요청을 처리하는 함수\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    for messages in messages_list:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {str(e)}\")\n",
    "            raise e\n",
    "    return responses\n",
    "\n",
    "def get_random_section(text, max_length):\n",
    "    \"\"\"\n",
    "    텍스트에서 랜덤한 섹션을 추출하는 함수\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "\n",
    "    # 시작 위치를 랜덤하게 선택\n",
    "    max_start = len(text) - max_length\n",
    "    start = random.randint(0, max_start)\n",
    "\n",
    "    # 문장이 중간에 잘리지 않도록 조정\n",
    "    # 마침표나 줄바꿈을 찾아서 적절한 위치에서 자르기\n",
    "    section = text[start:start + max_length]\n",
    "\n",
    "    # 첫 문장이 중간에 잘렸다면 다음 마침표까지 이동\n",
    "    first_period = section.find('.')\n",
    "    if first_period != -1:\n",
    "        section = section[first_period + 1:].strip()\n",
    "\n",
    "    # 마지막 문장이 중간에 잘렸다면 마지막 마침표까지만 포함\n",
    "    last_period = section.rfind('.')\n",
    "    if last_period != -1:\n",
    "        section = section[:last_period + 1].strip()\n",
    "\n",
    "    return section\n",
    "\n",
    "def process_in_batches(book_text, total_samples=10000):\n",
    "    texts = []\n",
    "    all_questions = []\n",
    "    all_responses = []\n",
    "\n",
    "    # 전체 배치 수 계산\n",
    "    num_batches = math.ceil(total_samples / BATCH_SIZE)\n",
    "\n",
    "    # 텍스트 샘플링\n",
    "    print(\"텍스트 샘플링 중...\")\n",
    "    for i, sample in tqdm(enumerate(total_samples)):\n",
    "        print(f\"샘플링 중... {i + 1}/{total_samples}\")\n",
    "        texts.append(get_random_section(sample, 2048))\n",
    "\n",
    "    # 배치 단위로 처리\n",
    "    print(\"\\n배치 처리 시작...\")\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = min((batch + 1) * BATCH_SIZE, total_samples)\n",
    "\n",
    "        try:\n",
    "            # 1. 질문 생성\n",
    "            batch_texts = texts[start_idx:end_idx]\n",
    "            qrys = []\n",
    "            for t in batch_texts:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": \"Your job is creating multi-hop reasoning questions in fluent Korean. You will be given a part of a text. Make a question based on it. The question should require multiple steps of reasoning related to the text. Return the question only without any other text\"},\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\": t}\n",
    "                ]\n",
    "                qrys.append(messages)\n",
    "\n",
    "            print(f\"\\n배치 {batch + 1}/{num_batches}: 질문 생성 중...\")\n",
    "            question_responses = batch_completion(qrys)\n",
    "            batch_questions = [r.choices[0].message.content for r in question_responses]\n",
    "\n",
    "            # 2. 답변 생성\n",
    "            qrys = []\n",
    "            for q in batch_questions:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\",\n",
    "                     \"content\": \"You are a skilled financial expert in Korea. Make a response for the question. DO NOT introduce yourself\"},\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\": q}\n",
    "                ]\n",
    "                qrys.append(messages)\n",
    "\n",
    "            print(f\"배치 {batch + 1}/{num_batches}: 답변 생성 중...\")\n",
    "            answer_responses = batch_completion(qrys)\n",
    "            batch_answers = [r.choices[0].message.content for r in answer_responses]\n",
    "\n",
    "            # 결과 저장\n",
    "            all_questions.extend(batch_questions)\n",
    "            all_responses.extend(batch_answers)\n",
    "\n",
    "            # 배치 중간 결과 저장\n",
    "            temp_df = pd.DataFrame({\n",
    "                'sampled_text': texts[:end_idx],\n",
    "                'question': all_questions,\n",
    "                'response': all_responses\n",
    "            })\n",
    "            temp_df.to_csv('qa_results_temp.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            print(f\"배치 {batch + 1} 완료. {DELAY}초 대기 중...\")\n",
    "            time.sleep(DELAY)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch}: {str(e)}\")\n",
    "            # 오류 발생 시 현재까지의 결과 저장\n",
    "            temp_df = pd.DataFrame({\n",
    "                'sampled_text': texts[:end_idx],\n",
    "                'question': all_questions,\n",
    "                'response': all_responses\n",
    "            })\n",
    "            temp_df.to_csv('qa_results_error.csv', index=False, encoding=\"utf-8-sig\")\n",
    "            raise e\n",
    "\n",
    "    # 최종 DataFrame 생성\n",
    "    final_df = pd.DataFrame({\n",
    "        'sampled_text': texts,\n",
    "        'question': all_questions,\n",
    "        'response': all_responses\n",
    "    })\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Google Drive 마운트\n",
    "        # drive.mount('/content/drive')\n",
    "\n",
    "        # 텍스트 파일 경로 설정 (예시)\n",
    "        file_path = 'allone.txt'  # 실제 파일 경로로 수정하세요\n",
    "\n",
    "        # 텍스트 파일 읽기\n",
    "        print(\"텍스트 파일 로딩 중...\")\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "            book_text = file.read()\n",
    "\n",
    "        print(\"처리 시작...\")\n",
    "        df = process_in_batches(book_text, total_samples=5000)\n",
    "\n",
    "        # 결과를 Google Drive에 저장\n",
    "        output_path = 'qa_results_final_10000.csv'\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"처리 완료. 결과가 저장됨: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Process stopped due to error: {str(e)}\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LogkdC8yuSsK",
   "metadata": {
    "id": "LogkdC8yuSsK"
   },
   "source": [
    "### 끝!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XWfU2WwJsb38",
   "metadata": {
    "id": "XWfU2WwJsb38"
   },
   "source": [
    "## 2. 함수 선언\n",
    "\n",
    "- 본 튜토리얼에서 사용하는 데이터셋인 `alvanlii/finance-textbooks`는 긴 길이의 텍스트 데이터로 구성되어 있기 때문에 적절한 길이의 데이터로 샘플링하는 작업이 필요합니다.\n",
    "- `get_random_section`함수는 매우 긴 텍스트에서 무작위로 일부분을 샘플링하기 위한 함수로, 이 함수를 활용하면 긴 텍스트 데이터에서 적절한 길이의 텍스트 데이터를 샘플링해서 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a80r95qcKL1t",
   "metadata": {
    "id": "a80r95qcKL1t"
   },
   "outputs": [],
   "source": [
    "def get_random_section(text, max_length=1000):\n",
    "    \"\"\"텍스트에서 랜덤한 섹션을 추출하는 함수\"\"\"\n",
    "    # 줄바꿈을 공백으로 대체하여 하나의 연속된 텍스트로 만듦\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "\n",
    "    start = random.randint(0, len(text) - max_length)\n",
    "\n",
    "    # 앞쪽으로 이동하여 마침표를 찾음\n",
    "    while start > 0 and text[start] != '.':\n",
    "        start -= 1\n",
    "    start = start + 1 if start > 0 else 0\n",
    "\n",
    "    section = text[start:start + max_length]\n",
    "\n",
    "    # 마지막 문장이 완성되도록 조정\n",
    "    last_period = section.rfind('.')\n",
    "    if last_period != -1:\n",
    "        section = section[:last_period + 1]\n",
    "\n",
    "    return section.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEscvx1vtDPI",
   "metadata": {
    "id": "zEscvx1vtDPI"
   },
   "source": [
    "## 3. 데이터셋 로드 및 전처리\n",
    "\n",
    "- `alvanlii/finance-textbooks` 데이터셋은 금융 관련 고품질 raw text 데이터셋으로 instruction 형태가 아닌 raw text로 구성되어 있습니다.\n",
    "- 데이터의 길이가 굉장히 길기 때문에 사전의 정의한 `get_random_section` 함수로 샘플링을 진행한 뒤 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1UhVwv3NH1jS",
   "metadata": {
    "id": "1UhVwv3NH1jS"
   },
   "outputs": [],
   "source": [
    "# TXT 파일 읽기\n",
    "with open('allone.txt', 'r', encoding='utf-8') as f:\n",
    "    book_text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NMsoiq7JXTqI",
   "metadata": {
    "id": "NMsoiq7JXTqI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 데이터셋 샘플링\n",
    "texts = []\n",
    "for i in range(10):  # 원하는 샘플 수만큼 조정\n",
    "    texts.append(get_random_section(book_text, 2048))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oLYbeE8Ptszz",
   "metadata": {
    "id": "oLYbeE8Ptszz"
   },
   "source": [
    "## 4. 합성 데이터셋 생성\n",
    "\n",
    "합성 데이터셋 생성 파이프라인은 다음과 같습니다.\n",
    "1. 샘플링된 raw text 데이터를 사용해서 질문 데이터셋을 생성합니다.\n",
    "2. 생성된 질문 데이터셋에 대한 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "FISvhwF5uc4V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FISvhwF5uc4V",
    "outputId": "bd4118ae-7a8f-4dd5-fdd0-527b7fdf4cb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `CompletionTokensDetails` but got `dict` with value `{'audio_tokens': None, 'reasoning_tokens': 0}` - serialized value may not be as expected\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': None, 'cached_tokens': 0}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# chat prompt 포맷팅\n",
    "qrys = []\n",
    "for t in texts:\n",
    "    messages = [\n",
    "    {\"content\":\"Your job is creating multi-hop reasoning questions in fluent Korean. You will be given a part of a text. Make a question based on it. The question should require multiple steps of reasoning related to the text. Return the question only without any other text.\",\"role\":\"system\"},\n",
    "    {\"content\": t, \"role\": \"user\"}]\n",
    "    qrys.append(messages)\n",
    "\n",
    "# 1. raw text 데이터를 활용한 질문 생성\n",
    "responses = batch_completion(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=qrys\n",
    ")\n",
    "resps = [i.choices[0].message.content for i in responses]\n",
    "total_prompt_tokens_for_q = sum([r.usage.prompt_tokens for r in responses])\n",
    "total_completion_tokens_for_q = sum([r.usage.completion_tokens for r in responses])\n",
    "df = pd.DataFrame({'sampled_text':texts,'question':resps})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ffebe8a-1b45-4ac8-876b-072ad413b3c7",
   "metadata": {
    "id": "4ffebe8a-1b45-4ac8-876b-072ad413b3c7"
   },
   "outputs": [],
   "source": [
    "# 답변 생성용 prompt 포맷팅\n",
    "qrys = []\n",
    "for t in resps:\n",
    "    messages = [\n",
    "    {\"content\":\"You are a skilled financial expert in Korea. Make a response for the question. DO NOT introduce yourself.\",\"role\":\"system\"},\n",
    "    {\"content\": t, \"role\": \"user\"}]\n",
    "    qrys.append(messages)\n",
    "\n",
    "# 2. 생성된 질문에 대한 답변 생성\n",
    "responses = batch_completion(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=qrys\n",
    ")\n",
    "resps = [i.choices[0].message.content for i in responses]\n",
    "df['response'] = resps\n",
    "total_prompt_tokens_for_a = sum([r.usage.prompt_tokens for r in responses])\n",
    "total_completion_tokens_for_a = sum([r.usage.completion_tokens for r in responses])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ENFxEQnuPi5",
   "metadata": {
    "id": "9ENFxEQnuPi5"
   },
   "source": [
    "## 5. 데이터셋 생성 비용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1da7c113-3214-46ea-97d4-bb73d4d7571e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1da7c113-3214-46ea-97d4-bb73d4d7571e",
    "outputId": "af689968-9261-4ba4-a546-1264e19ded42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompt tokens: 13576\n",
      "prompt token costs: $ 0.002036\n",
      "total completion tokens: 4681\n",
      "completion token costs: $ 0.002809\n"
     ]
    }
   ],
   "source": [
    "print('total prompt tokens:', total_prompt_tokens_for_q + total_prompt_tokens_for_a)\n",
    "print('prompt token costs: $', round((total_prompt_tokens_for_q + total_prompt_tokens_for_a) / 1000000 * 0.150, 6))\n",
    "print('total completion tokens:', total_completion_tokens_for_q + total_completion_tokens_for_a)\n",
    "print('completion token costs: $', round((total_completion_tokens_for_q + total_completion_tokens_for_a) / 1000000 * 0.600, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0VNB896iuatY",
   "metadata": {
    "id": "0VNB896iuatY"
   },
   "source": [
    "## 6. 데이터셋 저장 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c39ae9a-f9af-4f2d-bf08-ebd20042a708",
   "metadata": {
    "id": "5c39ae9a-f9af-4f2d-bf08-ebd20042a708"
   },
   "outputs": [],
   "source": [
    "# CSV 파일 저장\n",
    "df.to_csv(\"txt_result.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Excel 파일 저장\n",
    "# df.to_excel(\"content/txt_result.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LKmXsN7WLTiq",
   "metadata": {
    "id": "LKmXsN7WLTiq"
   },
   "outputs": [],
   "source": [
    "# HuggingFace Hub 업로드 - token에 개인 HuggingFace 토큰을 입력해주시면 됩니다.\n",
    "result_df = Dataset.from_pandas(df)\n",
    "result_df.push_to_hub(\"hf/dataset\", token=\"HF_TOKEN\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eOGRk0pUrcv1",
   "metadata": {
    "id": "eOGRk0pUrcv1"
   },
   "source": [
    "## 참고자료\n",
    "\n",
    "- [alvanlii/finance-textbooks](https://huggingface.co/datasets/alvanlii/finance-textbooks)\n",
    "- [litellm Docs](https://docs.litellm.ai/)\n",
    "- [Cosmopedia GitHub](https://github.com/huggingface/cosmopedia)\n",
    "- [Cosmopedia Blog](https://huggingface.co/blog/cosmopedia)\n",
    "- [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644)\n",
    "- [Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation](https://arxiv.org/abs/2402.18334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "KEkrwvMdOfyD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEkrwvMdOfyD",
    "outputId": "110a65f7-1a4f-4d32-b82f-66ccc572e23f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `CompletionTokensDetails` but got `dict` with value `{'audio_tokens': None, 'reasoning_tokens': 0}` - serialized value may not be as expected\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': None, 'cached_tokens': 1024}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  1%|          | 2/200 [00:31<51:43, 15.67s/it]/usr/local/lib/python3.10/dist-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `CompletionTokensDetails` but got `dict` with value `{'audio_tokens': None, 'reasoning_tokens': 0}` - serialized value may not be as expected\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': None, 'cached_tokens': 1280}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "  2%|▎         | 5/200 [01:18<49:06, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0mLiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  2%|▎         | 5/200 [01:21<52:49, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 5: 'RateLimitError' object has no attribute 'choices'\n",
      "Process stopped due to error: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm  # 진행 상황을 보여주는 프로그래스 바\n",
    "import math\n",
    "\n",
    "# 배치 크기 설정\n",
    "BATCH_SIZE = 25  # 한 번에 처리할 요청 수\n",
    "DELAY = 3  # 배치 간 대기 시간(초)\n",
    "\n",
    "def process_in_batches(total_samples=10000):\n",
    "    texts = []\n",
    "    all_questions = []\n",
    "    all_responses = []\n",
    "\n",
    "    # 전체 배치 수 계산\n",
    "    num_batches = math.ceil(total_samples / BATCH_SIZE)\n",
    "\n",
    "    # 텍스트 샘플링\n",
    "    for i in range(total_samples):\n",
    "        texts.append(get_random_section(book_text, 2048))\n",
    "\n",
    "    # 배치 단위로 처리\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = min((batch + 1) * BATCH_SIZE, total_samples)\n",
    "\n",
    "        try:\n",
    "            # 1. 질문 생성\n",
    "            batch_texts = texts[start_idx:end_idx]\n",
    "            qrys = []\n",
    "            for t in batch_texts:\n",
    "                messages = [\n",
    "                    {\"content\":\"Your job is creating multi-hop reasoning questions in fluent Korean. You will be given a part of a text. Make a question based on it. The question should require multiple steps of reasoning related to the text. Return the question only without any other text\",\n",
    "                     \"role\":\"system\"},\n",
    "                    {\"content\": t, \"role\": \"user\"}\n",
    "                ]\n",
    "                qrys.append(messages)\n",
    "\n",
    "            question_responses = batch_completion(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=qrys\n",
    "            )\n",
    "            batch_questions = [r.choices[0].message.content for r in question_responses]\n",
    "\n",
    "            # 2. 답변 생성\n",
    "            qrys = []\n",
    "            for q in batch_questions:\n",
    "                messages = [\n",
    "                    {\"content\":\"You are a skilled financial expert in Korea. Make a response for the question. DO NOT introduce yourself\",\n",
    "                     \"role\":\"system\"},\n",
    "                    {\"content\": q, \"role\": \"user\"}\n",
    "                ]\n",
    "                qrys.append(messages)\n",
    "\n",
    "            answer_responses = batch_completion(\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                messages=qrys\n",
    "            )\n",
    "            batch_answers = [r.choices[0].message.content for r in answer_responses]\n",
    "\n",
    "            # 결과 저장\n",
    "            all_questions.extend(batch_questions)\n",
    "            all_responses.extend(batch_answers)\n",
    "\n",
    "            # 배치 중간 결과 저장\n",
    "            temp_df = pd.DataFrame({\n",
    "                'sampled_text': texts[:end_idx],\n",
    "                'question': all_questions,\n",
    "                'response': all_responses\n",
    "            })\n",
    "            temp_df.to_csv('qa_results_temp.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            # 배치 간 대기\n",
    "            time.sleep(DELAY)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch}: {str(e)}\")\n",
    "            # 오류 발생 시 현재까지의 결과 저장\n",
    "            temp_df = pd.DataFrame({\n",
    "                'sampled_text': texts[:end_idx],\n",
    "                'question': all_questions,\n",
    "                'response': all_responses\n",
    "            })\n",
    "            temp_df.to_csv('qa_results_error.csv', index=False, encoding=\"utf-8-sig\")\n",
    "            raise e\n",
    "\n",
    "    # 최종 DataFrame 생성\n",
    "    final_df = pd.DataFrame({\n",
    "        'sampled_text': texts,\n",
    "        'question': all_questions,\n",
    "        'response': all_responses\n",
    "    })\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# 실행\n",
    "try:\n",
    "    df = process_in_batches(total_samples=10000)\n",
    "    df.to_csv('qa_results_final_10000.csv', index=False, encoding=\"utf-8-sig\")\n",
    "except Exception as e:\n",
    "    print(f\"Process stopped due to error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ZhGx5hyPhgz",
   "metadata": {
    "id": "2ZhGx5hyPhgz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
